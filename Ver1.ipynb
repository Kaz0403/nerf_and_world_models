{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import random\n","from torch import nn, optim"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Get_data():\n","    def __init__(self, t):\n","        data_f = \"test (1).npz\"\n","        data = np.load(data_f)\n","\n","        images = data[\"images\"][t] / 255\n","        img_size = images.shape[1]\n","        xs = torch.arange(img_size) - (img_size / 2 - 0.5)\n","        ys = torch.arange(img_size) - (img_size / 2 - 0.5)\n","        (xs, ys) = torch.meshgrid(xs, -ys, indexing=\"xy\")\n","        focal = float(data[\"focal\"])\n","        pixel_coords = torch.stack([xs, ys, torch.full_like(xs, -focal)], dim=-1)\n","        camera_coords = pixel_coords / focal\n","        init_ds = camera_coords.to(device)\n","        init_o = torch.Tensor(np.array([0, 0, float(data[\"cam_dist\"])])).to(device)\n","\n","        test_idx = 30\n","        plt.imshow(images[test_idx])\n","        plt.show()\n","        test_img = torch.Tensor(images[test_idx]).to(device)\n","        poses = torch.Tensor(data[\"poses\"].reshape((-1,4,4))).to(device)\n","        test_pose = torch.Tensor(poses[test_idx]).to(device)\n","        test_R = torch.Tensor(poses[test_idx, :3, :3]).to(device)\n","        test_ds = torch.einsum(\"ij,hwj->hwi\", test_R, init_ds)\n","        test_os = (test_R @ init_o).expand(test_ds.shape)\n","\n","        train_idxs = np.arange(len(images)) != test_idx # test_idxだけ除去(1枚)\n","        images = torch.Tensor(images[train_idxs])\n","        poses = torch.Tensor(poses[train_idxs])\n","\n","        return images, poses, init_ds, init_o, test_img, test_pose, test_os, test_ds"]},{"cell_type":"markdown","metadata":{},"source":["ResNet18"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ResNet18\n","def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(BasicBlock, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if groups != 1 or base_width != 64:\n","            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n","        if dilation > 1:\n","            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n","        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = norm_layer(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = norm_layer(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","class ResNet(nn.Module):\n","\n","    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n","                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n","                 norm_layer=None):\n","        super(ResNet, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","\n","        self.inplanes = 64\n","        self.dilation = 1\n","        if replace_stride_with_dilation is None:\n","            # each element in the tuple indicates if we should replace\n","            # the 2x2 stride with a dilated convolution instead\n","            replace_stride_with_dilation = [False, False, False]\n","        if len(replace_stride_with_dilation) != 3:\n","            raise ValueError(\"replace_stride_with_dilation should be None \"\n","                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n","        self.groups = groups\n","        self.base_width = width_per_group\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = norm_layer(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n","                                       dilate=replace_stride_with_dilation[0])\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n","                                       dilate=replace_stride_with_dilation[1])\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n","                                       dilate=replace_stride_with_dilation[2])\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","        # self.fc = nn.Linear(2048*4, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                # if isinstance(m, Bottleneck):\n","                #     nn.init.constant_(m.bn3.weight, 0)\n","                # elif isinstance(m, BasicBlock):\n","                nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n","        norm_layer = self._norm_layer\n","        downsample = None\n","        previous_dilation = self.dilation\n","        if dilate:\n","            self.dilation *= stride\n","            stride = 1\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                norm_layer(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n","                            self.base_width, previous_dilation, norm_layer))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, groups=self.groups,\n","                                base_width=self.base_width, dilation=self.dilation,\n","                                norm_layer=norm_layer))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _forward_impl(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x)\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def forward(self, x):\n","        return self._forward_impl(x)"]},{"cell_type":"markdown","metadata":{},"source":["Vt獲得"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class get_vt(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.resnet18 = ResNet(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=self.num_classes) # 出力は256次元\n","\n","        # Vi と　camera matrix を結合したものに対するMLP\n","        self.cat_cam_mat_mlp = nn.Sequential(\n","            nn.Linear(self.cat_cam_mat_feats, self.net_width),\n","            nn.ReLU(),\n","            nn.Linear(self.net_width, self.net_width),\n","            nn.ReLU(),\n","        )\n","\n","    def forward(self, images, poses):\n","        vt = []\n","        for i,img in enumerate(images):\n","            outputs_resnet = self.resnet18(img.unsqueeze(0))\n","            cat_cam_mat = torch.cat((outputs_resnet, torch.flatten(poses[i])), dim=-1)\n","            outputs_cat_cam_mat = self.cat_cam_mat_mlp(cat_cam_mat) # これをすべての視点から集めたい\n","            vt.append(outputs_cat_cam_mat)\n","        # print(vt)\n","        \n","        vt = torch.stack(vt, dim=0)\n","        return vt\n","\n","        "]},{"cell_type":"markdown","metadata":{},"source":["Loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def ContrastiveLoss(model, images, poses, v):\n","    while True:\n","        i = random.randint(0, len(images))\n","        j = random.randint(0, len(images))\n","        if i != j:\n","            break\n","\n","    loss = (((model(images, poses)[i] - model(images, poses)[j]) ** 2 - ((model(images,poses)[i] - v[i]) ** 2)).max()) #αがわからないのでpass\n","    if loss < 0: return 0\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["images, poses, init_ds, init_o, test_img, test_pose, test_os, test_ds = Get_data(t) # 時刻t　ここどうしよう"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = get_vt(images, poses)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","for t in range(2000):\n","    loss = ContrastiveLoss(model, images, poses, v) # 過去のvも必要\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class get_st(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.num_classes = 256\n","        self.cat_cam_mat_feats = self.num_classes + 16\n","        self.net_width = 256\n","        self.resnet18 = ResNet(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=self.num_classes) # 出力は256次元\n","        \n","        # 上で得られたものに対して平均を取ったものにかけるMLP\n","        self.vt_to_st = nn.Sequential( \n","            nn.Linear(self.net_width, self.net_width),\n","            nn.ReLU(),\n","            nn.Linear(self.net_width, self.net_width),\n","            nn.ReLU(),\n","        )\n","    def forward(self, images, poses):\n","        vt = get_vt(images, poses)\n","        vt_avg = torch.mean(vt, dim=0)\n","        return self.vt_to_st(vt_avg)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_coarse_query_points(ds, N_c, t_i_c_bin_edges, t_i_c_gap, os): #粗いレンダリングのためのpoint取得\n","    u_is_c = torch.rand(*list(ds.shape[:2]) + [N_c]).to(ds)\n","    t_is_c = t_i_c_bin_edges + u_is_c * t_i_c_gap\n","    r_ts_c = os[..., None, :] + t_is_c[..., :, None] * ds[..., None, :]\n","    return (r_ts_c, t_is_c)\n","\n","\n","def render_radiance_volume(img, target_pose, r_ts, ds, chunk_size, F, t_is): #画像のレンダリング(再構成)\n","    r_ts_flat = r_ts.reshape((-1, 3))\n","    ds_rep = ds.unsqueeze(2).repeat(1, 1, r_ts.shape[-2], 1)\n","    ds_flat = ds_rep.reshape((-1, 3))\n","    c_is = []\n","    sigma_is = []\n","    for chunk_start in range(0, r_ts_flat.shape[0], chunk_size):\n","        r_ts_batch = r_ts_flat[chunk_start : chunk_start + chunk_size]\n","        ds_batch = ds_flat[chunk_start : chunk_start + chunk_size]\n","        preds = F(img, target_pose, r_ts_batch, ds_batch) # ここで実行してる\n","        c_is.append(preds[\"c_is\"]) #実行して得られたRGB\n","        sigma_is.append(preds[\"sigma_is\"]) #実行して得られたσ\n","\n","    c_is = torch.cat(c_is).reshape(r_ts.shape) \n","    sigma_is = torch.cat(sigma_is).reshape(r_ts.shape[:-1])\n","\n","    delta_is = t_is[..., 1:] - t_is[..., :-1] #微小区間\n","    one_e_10 = torch.Tensor([1e10]).expand(delta_is[..., :1].shape)\n","    delta_is = torch.cat([delta_is, one_e_10.to(delta_is)], dim=-1)\n","    delta_is = delta_is * ds.norm(dim=-1).unsqueeze(-1)\n","\n","    alpha_is = 1.0 - torch.exp(-sigma_is * delta_is)\n","\n","    T_is = torch.cumprod(1.0 - alpha_is + 1e-10, -1)\n","    T_is = torch.roll(T_is, 1, -1)\n","    T_is[..., 0] = 1.0\n","\n","    w_is = T_is * alpha_is\n","\n","    C_rs = (w_is[..., None] * c_is).sum(dim=-2)\n","\n","    return C_rs\n","\n","\n","def run_one_iter_of_tiny_nerf(img, target_pose, ds, N_c, t_i_c_bin_edges, t_i_c_gap, os, chunk_size, F_c): #任意の視点の画像を得る\n","    (r_ts_c, t_is_c) = get_coarse_query_points(ds, N_c, t_i_c_bin_edges, t_i_c_gap, os)\n","    C_rs_c = render_radiance_volume(img, target_pose, r_ts_c, ds, chunk_size, F_c, t_is_c) # ここで実行してる\n","    return C_rs_c\n","\n","\n","class VeryTinyNeRFMLP(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.L_pos = 6\n","        self.L_dir = 4\n","        num_classes = 256\n","        pos_enc_feats = 3 + 3 * 2 * self.L_pos + net_width\n","        dir_enc_feats = 3 + 3 * 2 * self.L_dir\n","\n","        net_width = 256\n","        self.resnet18 = ResNet(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=num_classes) # 出力は256次元\n","        \n","        \n","        # NeRFのσを得るまでのMLP\n","        self.early_mlp = nn.Sequential(\n","            nn.Linear(pos_enc_feats, net_width),\n","            nn.ReLU(),\n","            nn.Linear(net_width, net_width + 1), # なんで+1？ この+1された部分がσなんだと思います\n","            nn.ReLU(),\n","        )\n","        # NeRFのRGBを得るまでのMLP\n","        self.late_mlp = nn.Sequential(\n","            nn.Linear(net_width + dir_enc_feats, net_width),\n","            nn.ReLU(),\n","            nn.Linear(net_width, 3),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, images, poses, xs, ds):\n","        # print(xs.shape)\n","\n","        #位置xのpositional encoding\n","        xs_encoded = [xs] \n","        for l_pos in range(self.L_pos):\n","            xs_encoded.append(torch.sin(2 ** l_pos * torch.pi * xs))\n","            xs_encoded.append(torch.cos(2 ** l_pos * torch.pi * xs))\n","        xs_encoded = torch.cat(xs_encoded, dim=-1) #12次元？\n","        # print(ds.shape)\n","\n","        #角度dのpositional encodeing\n","        ds = ds / ds.norm(p=2, dim=-1).unsqueeze(-1)\n","        ds_encoded = [ds]\n","        for l_dir in range(self.L_dir):\n","            ds_encoded.append(torch.sin(2 ** l_dir * torch.pi * ds))\n","            ds_encoded.append(torch.cos(2 ** l_dir * torch.pi * ds))\n","        ds_encoded = torch.cat(ds_encoded, dim=-1) #8次元?\n","        \n","        st = get_st(images, poses)\n","\n","        # print(outputs_cat_cam_mat.shape)\n","        xs_encoded = torch.cat((xs_encoded, st.unsqueeze(0).repeat(xs_encoded.shape[0], 1)), dim=-1) # xs_encodedはどんな形してるか\n","        outputs = self.early_mlp(xs_encoded)\n","        sigma_is = outputs[:, 0]\n","        c_is = self.late_mlp(torch.cat([outputs[:, 1:], ds_encoded], dim=-1))\n","        return {\"c_is\": c_is, \"sigma_is\": sigma_is}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seed = 9458\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","\n","device = \"cuda:0\"\n","F_c = VeryTinyNeRFMLP()\n","chunk_size = 16384\n","\n","lr = 5e-3\n","optimizer = optim.Adam(list(F_c.parameters()) + list(get_st.parameters()), lr=lr)\n","criterion = nn.MSELoss()\n","\n","t_n = 1.0\n","t_f = 4.0\n","N_c = 32\n","t_i_c_gap = (t_f - t_n) / N_c\n","t_i_c_bin_edges = (t_n + torch.arange(N_c) * t_i_c_gap).to(device)\n","\n","psnrs = []\n","iternums = []\n","num_iters = 20000\n","display_every = 100\n","F_c.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(num_iters):\n","    target_img_idx = np.random.randint(images.shape[0])\n","    target_pose = poses[target_img_idx].to(device)\n","    R = target_pose[:3, :3]\n","\n","    ds = torch.einsum(\"ij,hwj->hwi\", R, init_ds)\n","    os = (R @ init_o).expand(ds.shape)\n","\n","    C_rs_c = run_one_iter_of_tiny_nerf(\n","        images[target_img_idx].unsqueeze(0).permute(0,3,1,2).to(device), poses, ds, N_c, t_i_c_bin_edges, t_i_c_gap, os, chunk_size, F_c\n","    )\n","    loss = criterion(C_rs_c, images.to(device))\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if i % display_every == 0:\n","        F_c.eval()\n","        with torch.no_grad():\n","            C_rs_c = run_one_iter_of_tiny_nerf(\n","                test_img.unsqueeze(0).permute(0,3,1,2).to(device), test_pose, test_ds, N_c, t_i_c_bin_edges, t_i_c_gap, test_os, chunk_size, F_c\n","            )\n","\n","        loss = criterion(C_rs_c, test_img)\n","        print(f\"Loss: {loss.item()}\")\n","        psnr = -10.0 * torch.log10(loss)\n","\n","        psnrs.append(psnr.item())\n","        iternums.append(i)\n","\n","        plt.figure(figsize=(10, 4))\n","        plt.subplot(121)\n","        plt.imshow(C_rs_c.detach().cpu().numpy())\n","        plt.title(f\"Iteration {i}\")\n","        plt.subplot(122)\n","        plt.plot(iternums, psnrs)\n","        plt.title(\"PSNR\")\n","        plt.show()\n","\n","        F_c.train()\n","\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["st = get_st(images, poses)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPsD3Aa1gjRMC3INQPAIJ6I","mount_file_id":"1k4YLcf1EMlKH9mmNC1mSenlGjRc57L_u","provenance":[]},"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
