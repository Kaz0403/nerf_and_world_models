{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05wDHgnbiXHq"
   },
   "source": [
    "See my notebook [here](https://colab.research.google.com/drive/1MY6pk3vY7rrYal8oS6_s7zTkGN9lkHQr?usp=sharing) demonstrating how to use my code to train a NeRF model on the `tiny_nerf_data.npz` file used by the original NeRF authors in their notebook [here](https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKJ6NMd642-Y",
    "outputId": "8c52c241-c870-4caf-f1be-c1184449d930"
   },
   "outputs": [],
   "source": [
    "!pip install -q -q -q matplotlib numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "eyiZCUkSW5HH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "# ResNet18\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        # self.fc = nn.Linear(2048*4, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                # if isinstance(m, Bottleneck):\n",
    "                #     nn.init.constant_(m.bn3.weight, 0)\n",
    "                # elif isinstance(m, BasicBlock):\n",
    "                nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_classes = 256\n",
    "        cat_cam_mat_feats = num_classes + 16 # 視点数 16\n",
    "        net_width = 256\n",
    "        self.resnet18 = ResNet(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=num_classes) # 出力は256次元\n",
    "        self.cat_cam_mat_mlp = nn.Sequential(\n",
    "            nn.Linear(cat_cam_mat_feats, net_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(net_width, net_width),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, images_t, poses):\n",
    "        vt = []\n",
    "        for i,img in enumerate(images_t):\n",
    "            outputs_resnet = self.resnet18(img.unsqueeze(0))\n",
    "            cat_cam_mat = torch.cat((outputs_resnet, torch.flatten(poses[i])), dim=-1)\n",
    "            # print(torch.flatten(poses[i]).shape)\n",
    "            outputs_cat_cam_mat = self.cat_cam_mat_mlp(cat_cam_mat) # これをすべての視点から集めたい\n",
    "            vt.append(outputs_cat_cam_mat)\n",
    "        vt = torch.stack(vt)\n",
    "        return vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        net_width = 256\n",
    "        self.vt_to_st = nn.Sequential(\n",
    "            nn.Linear(net_width, net_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(net_width, net_width),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, vt):\n",
    "        vt_avg = torch.mean(vt, dim=0) # dim合ってる？\n",
    "        st = self.vt_to_st(vt_avg)\n",
    "        return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeryTinyNeRFMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.L_pos = 6\n",
    "        self.L_dir = 4\n",
    "        num_classes = 256\n",
    "        pos_enc_feats = 3 + 3 * 2 * self.L_pos + num_classes + 16\n",
    "        dir_enc_feats = 3 + 3 * 2 * self.L_dir\n",
    "        net_width = 256\n",
    "   \n",
    "        self.early_mlp = nn.Sequential(\n",
    "            nn.Linear(net_width + self.L_pos*2*3+3, net_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(net_width, net_width + 1), # なんで+1？\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.late_mlp = nn.Sequential(\n",
    "            nn.Linear(net_width + dir_enc_feats, net_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(net_width, 3),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, st, images_t, target_idx, poses, xs, ds):\n",
    "        xs_encoded = [xs]\n",
    "        for l_pos in range(self.L_pos):\n",
    "            xs_encoded.append(torch.sin(2 ** l_pos * torch.pi * xs))\n",
    "            xs_encoded.append(torch.cos(2 ** l_pos * torch.pi * xs))\n",
    "\n",
    "        xs_encoded = torch.cat(xs_encoded, dim=-1)\n",
    "\n",
    "        ds = ds / ds.norm(p=2, dim=-1).unsqueeze(-1)\n",
    "        ds_encoded = [ds]\n",
    "        for l_dir in range(self.L_dir):\n",
    "            ds_encoded.append(torch.sin(2 ** l_dir * torch.pi * ds))\n",
    "            ds_encoded.append(torch.cos(2 ** l_dir * torch.pi * ds))\n",
    "\n",
    "        ds_encoded = torch.cat(ds_encoded, dim=-1)\n",
    "        xs_encoded = torch.cat((xs_encoded, st.unsqueeze(0).repeat(xs_encoded.shape[0], 1)), dim=-1) # xs_encodedがどんな形してるか？\n",
    "        \n",
    "        outputs = self.early_mlp(xs_encoded)\n",
    "        sigma_is = outputs[:, 0]\n",
    "        c_is = self.late_mlp(torch.cat([outputs[:, 1:], ds_encoded], dim=-1))\n",
    "        return {\"c_is\": c_is, \"sigma_is\": sigma_is}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coarse_query_points(ds, N_c, t_i_c_bin_edges, t_i_c_gap, os):\n",
    "    u_is_c = torch.rand(*list(ds.shape[:2]) + [N_c]).to(ds)\n",
    "    t_is_c = t_i_c_bin_edges + u_is_c * t_i_c_gap\n",
    "    r_ts_c = os[..., None, :] + t_is_c[..., :, None] * ds[..., None, :]\n",
    "    return (r_ts_c, t_is_c)\n",
    "\n",
    "\n",
    "def render_radiance_volume(images_t, target_idx, poses, r_ts, ds, chunk_size, F_img_enc, F_st_enc, F_dec, t_is):\n",
    "    r_ts_flat = r_ts.reshape((-1, 3))\n",
    "    ds_rep = ds.unsqueeze(2).repeat(1, 1, r_ts.shape[-2], 1)\n",
    "    ds_flat = ds_rep.reshape((-1, 3))\n",
    "    c_is = []\n",
    "    sigma_is = []\n",
    "    for chunk_start in range(0, r_ts_flat.shape[0], chunk_size):\n",
    "        r_ts_batch = r_ts_flat[chunk_start : chunk_start + chunk_size]\n",
    "        ds_batch = ds_flat[chunk_start : chunk_start + chunk_size]\n",
    "        vt = F_img_enc(images_t, poses)\n",
    "        st = F_st_enc(vt)\n",
    "        preds = F_dec(st, images, target_idx, poses, r_ts_batch, ds_batch) # ここで実行してる？\n",
    "        c_is.append(preds[\"c_is\"])\n",
    "        sigma_is.append(preds[\"sigma_is\"])\n",
    "\n",
    "    c_is = torch.cat(c_is).reshape(r_ts.shape)\n",
    "    sigma_is = torch.cat(sigma_is).reshape(r_ts.shape[:-1])\n",
    "\n",
    "    delta_is = t_is[..., 1:] - t_is[..., :-1]\n",
    "    one_e_10 = torch.Tensor([1e10]).expand(delta_is[..., :1].shape)\n",
    "    delta_is = torch.cat([delta_is, one_e_10.to(delta_is)], dim=-1)\n",
    "    delta_is = delta_is * ds.norm(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    alpha_is = 1.0 - torch.exp(-sigma_is * delta_is)\n",
    "\n",
    "    T_is = torch.cumprod(1.0 - alpha_is + 1e-10, -1)\n",
    "    T_is = torch.roll(T_is, 1, -1)\n",
    "    T_is[..., 0] = 1.0\n",
    "\n",
    "    w_is = T_is * alpha_is\n",
    "\n",
    "    C_rs = (w_is[..., None] * c_is).sum(dim=-2)\n",
    "\n",
    "    return C_rs, vt\n",
    "\n",
    "\n",
    "def run_one_iter_of_tiny_nerf(images_t, target_idx, poses, ds, N_c, t_i_c_bin_edges, t_i_c_gap, os, chunk_size, F_img_enc, F_st_enc, F_dec):\n",
    "    (r_ts_c, t_is_c) = get_coarse_query_points(ds, N_c, t_i_c_bin_edges, t_i_c_gap, os)\n",
    "    C_rs_c, vt = render_radiance_volume(images_t, target_idx, poses, r_ts_c, ds, chunk_size, F_img_enc, F_st_enc, F_dec, t_is_c) # ここで実行してる？\n",
    "    return C_rs_c, vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeContrastiveLoss(nn.Module):\n",
    "    def __init__(self): # パラメータの設定など初期化処理を行う\n",
    "        super(TimeContrastiveLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, vt_prevs, vt, poses, alpha=0):\n",
    "        while True:\n",
    "            i = random.randint(0, poses.shape[0]-1)\n",
    "            j = random.randint(0, poses.shape[0]-1)\n",
    "            if i != j:\n",
    "                break\n",
    "                \n",
    "        # とりあえず1時刻前の対照学習しかしない\n",
    "        loss = torch.norm((vt[i] - vt[j]) ** 2, 2) - torch.norm((vt[i] - vt_prevs[-1][i]) ** 2, 2) + alpha\n",
    "        if loss < 0: return 0\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rgMPFZ7WXDQ3",
    "outputId": "4d71a3e4-23ca-46f3-f037-966c6b5f793b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VeryTinyNeRFMLP(\n",
       "  (early_mlp): Sequential(\n",
       "    (0): Linear(in_features=295, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=257, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (late_mlp): Sequential(\n",
       "    (0): Linear(in_features=283, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=3, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 9458\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "F_image_enc = ImageEncoder().to(device)\n",
    "F_state_enc = StateEncoder().to(device)\n",
    "F_dec = VeryTinyNeRFMLP().to(device)\n",
    "chunk_size = 16384\n",
    "\n",
    "lr = 5e-3\n",
    "optimizer_image_enc = optim.Adam(F_image_enc.parameters(), lr=lr)\n",
    "optimizer_state_enc = optim.Adam(F_state_enc.parameters(), lr=lr)\n",
    "optimizer_dec = optim.Adam(F_dec.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "criterion_TCN = TimeContrastiveLoss()\n",
    "\n",
    "data_f = \"test (4).npz\"\n",
    "data = np.load(data_f)\n",
    "\n",
    "images = data[\"images\"] / 255\n",
    "images_t0 = images[0]\n",
    "img_size = images_t0.shape[1]\n",
    "xs = torch.arange(img_size) - (img_size / 2 - 0.5)\n",
    "ys = torch.arange(img_size) - (img_size / 2 - 0.5)\n",
    "(xs, ys) = torch.meshgrid(xs, -ys, indexing=\"xy\")\n",
    "focal = float(data[\"focal\"])\n",
    "pixel_coords = torch.stack([xs, ys, torch.full_like(xs, -focal)], dim=-1)\n",
    "camera_coords = pixel_coords / focal\n",
    "init_ds = camera_coords.to(device)\n",
    "init_o = torch.Tensor(np.array([0, 0, float(data[\"cam_dist\"])])).to(device)\n",
    "\n",
    "poses = torch.Tensor(data[\"poses\"].reshape((-1,4,4))).to(device)\n",
    "\n",
    "t_n = 1.0 # near\n",
    "t_f = 6.0 # far\n",
    "N_c = 32\n",
    "t_i_c_gap = (t_f - t_n) / N_c\n",
    "t_i_c_bin_edges = (t_n + torch.arange(N_c) * t_i_c_gap).to(device)\n",
    "\n",
    "psnrs = []\n",
    "iternums = []\n",
    "num_iters = 60\n",
    "display_every = 30\n",
    "F_image_enc.train()\n",
    "F_state_enc.train()\n",
    "F_dec.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t6yJtLhVXKfG",
    "outputId": "c51e5874-af16-445f-b9df-7a301eba5efa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vt_prevs = []\n",
    "for t in range(images.shape[0]):\n",
    "    test_idx = random.randint(0, poses.shape[0])\n",
    "    train_idxs = np.arange(len(images_t0)) != test_idx # test_idxだけ除去(1枚)\n",
    "    images_t = torch.Tensor(images[t][train_idxs]).to(device)\n",
    "    poses_t = torch.Tensor(poses[train_idxs]).to(device)\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        target_img_idx = np.random.randint(images_t.shape[0])\n",
    "        target_pose = poses[target_img_idx].to(device)\n",
    "        R = target_pose[:3, :3]\n",
    "\n",
    "        ds = torch.einsum(\"ij,hwj->hwi\", R, init_ds)\n",
    "        os = (R @ init_o).expand(ds.shape)\n",
    "\n",
    "        C_rs_c, vt = run_one_iter_of_tiny_nerf(\n",
    "            images_t.permute(0,3,1,2).to(device), target_img_idx, poses, ds, N_c, t_i_c_bin_edges, t_i_c_gap, os, chunk_size, F_image_enc, F_state_enc, F_dec\n",
    "        )\n",
    "        loss_img_enc = criterion(C_rs_c, images_t.to(device))\n",
    "        optimizer_image_enc.zero_grad()\n",
    "        loss_img_enc.backward()\n",
    "        optimizer_image_enc.step()\n",
    "\n",
    "        if (i+1) % display_every == 0:\n",
    "            test_img = torch.Tensor(images_t[test_idx]).to(device)\n",
    "            test_pose = torch.Tensor(poses[test_idx]).unsqueeze(0).to(device)\n",
    "            F_image_enc.eval()\n",
    "            F_state_enc.eval()\n",
    "            F_dec.eval()\n",
    "            with torch.no_grad():\n",
    "                C_rs_c, vt_test = run_one_iter_of_tiny_nerf(\n",
    "                    test_img.unsqueeze(0).permute(0,3,1,2).to(device), test_idx, test_pose, test_ds, N_c, t_i_c_bin_edges, t_i_c_gap, test_os, chunk_size, F_image_enc, F_state_enc, F_dec\n",
    "                )\n",
    "\n",
    "            loss_dec = criterion(C_rs_c, test_img)\n",
    "            print(f\"Time Step: {t}\")\n",
    "            print(f\"Iter: {i}\")\n",
    "            print(f\"Loss: {loss_img_enc.item()}\")\n",
    "\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.imshow(C_rs_c.detach().cpu().numpy())\n",
    "            plt.show()\n",
    "            \n",
    "            # チェックポイント保存\n",
    "            file_name = str(t) + '_' + str(i) + '.cpt'\n",
    "            torch.save({\n",
    "            'time': t,\n",
    "            'iter': i,\n",
    "            'image_encoder_state_dict': F_image_enc.state_dict(), # vtまでのencoder\n",
    "            'state_encoder_state_dict': F_state_enc.state_dict(), # vtまでのencoder\n",
    "            'decoder_state_dict': F_dec.state_dict(),\n",
    "            'opt_img_enc_state_dict': optimizer_image_enc.state_dict(),\n",
    "            'opt_dec_state_dict': optimizer_dec.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, file_name)\n",
    "\n",
    "            F_image_enc.train()\n",
    "            F_state_enc.train()\n",
    "            F_dec.train()\n",
    "    \n",
    "    # どこで学習させるべき？\n",
    "    if t != 0:\n",
    "        loss_dec = criterion_TCN(vt_prevs, vt, poses)\n",
    "        print(loss_dec)\n",
    "        if loss_dec > 0:\n",
    "            print('ok')\n",
    "            optimizer_dec.zero_grad()\n",
    "            loss_dec.backward()\n",
    "            optimizer_image_enc.step()\n",
    "        \n",
    "    # 前の時刻のvtを保存\n",
    "    if len(vt_prevs) == 0:\n",
    "        vt_prevs = vt\n",
    "    else:\n",
    "        vt_prevs = torch.cat((vt_prevs, vt), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
