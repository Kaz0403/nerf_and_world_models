{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05wDHgnbiXHq"
      },
      "source": [
        "See my notebook [here](https://colab.research.google.com/drive/1MY6pk3vY7rrYal8oS6_s7zTkGN9lkHQr?usp=sharing) demonstrating how to use my code to train a NeRF model on the `tiny_nerf_data.npz` file used by the original NeRF authors in their notebook [here](https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kKJ6NMd642-Y"
      },
      "outputs": [],
      "source": [
        "!pip install -q matplotlib numpy torch natsort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuLuxmahx0Qs",
        "outputId": "d245c9e3-9c0f-447f-8544-07449e1b3511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U einops datasets tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eyiZCUkSW5HH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "from natsort import natsorted\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch import nn, optim, einsum\n",
        "\n",
        "from einops import rearrange, reduce\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "import math\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sOHik67Cx0Qt"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NxX57mqIx0Qt"
      },
      "outputs": [],
      "source": [
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), \n",
        "                                    nn.GroupNorm(1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "        return self.to_out(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BEJ2QHOex0Qu"
      },
      "outputs": [],
      "source": [
        "# ResNet18\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, down_sample=False, groups=1,\n",
        "                 base_width=64, dilation=1, time_dim=None, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "\n",
        "        self.scale_shift_mlp = (\n",
        "            nn.Sequential(nn.SiLU(), nn.Linear(time_dim, planes * 2))\n",
        "        )\n",
        "                \n",
        "        self.inplanes = inplanes\n",
        "        self.planes = planes\n",
        "        self.down_sample = down_sample\n",
        "\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.conv1x1 = conv1x1(self.inplanes, self.planes, stride)\n",
        "        \n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "\n",
        "        #scale_shift\n",
        "        time_emb = self.scale_shift_mlp(time_emb)\n",
        "        time_emb = rearrange(time_emb, \"b c -> b c 1 1\")\n",
        "        scale_shift = time_emb.chunk(2, dim=1)\n",
        "\n",
        "\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "\n",
        "        scale, shift = scale_shift\n",
        "        out = out * (scale + 1) + shift\n",
        "\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.down_sample == True:\n",
        "          identity = self.conv1x1(identity)\n",
        "          identity = self.bn1(identity)\n",
        "\n",
        "        #print(out.shape, identity.shape)\n",
        "        out += identity #残差結合\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self.norm_layer = norm_layer\n",
        "\n",
        "        #入力チャンネル\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        self.time_dim = self.inplanes * 4\n",
        "\n",
        "        #if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "        #    replace_stride_with_dilation = [False, False, False]\n",
        "        #if len(replace_stride_with_dilation) != 3:\n",
        "        #    raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "        #                     \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "\n",
        "        #Time_Embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(self.inplanes),\n",
        "            nn.Linear(self.inplanes, self.time_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(self.time_dim, self.time_dim),\n",
        "        )\n",
        "\n",
        "        #畳み込み1\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        #Attention\n",
        "        self.attn = LinearAttention(self.inplanes)\n",
        "\n",
        "        #正則化\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "\n",
        "        #活性化\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        #プーリング\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        #ResNet ブロック\n",
        "        self.layer1 = block(inplanes = self.inplanes, planes = 64, norm_layer = self.norm_layer, time_dim=self.time_dim)\n",
        "            \n",
        "        self.layer2 = block(inplanes = 64, planes = 128, stride=2, down_sample = True, norm_layer = self.norm_layer, time_dim=self.time_dim)\n",
        "\n",
        "        self.layer3 = block(inplanes = 128, planes = 256, stride=2, down_sample = True, norm_layer = self.norm_layer, time_dim=self.time_dim)\n",
        "\n",
        "        self.layer4 = block(inplanes = 256, planes = 512, stride=2, down_sample = True, norm_layer = self.norm_layer, time_dim=self.time_dim)\n",
        "        \n",
        "        #プーリング\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        #全結合\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "        # self.fc = nn.Linear(2048*4, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                # if isinstance(m, Bottleneck):\n",
        "                #     nn.init.constant_(m.bn3.weight, 0)\n",
        "                # elif isinstance(m, BasicBlock):\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "\n",
        "    def _forward_impl(self, x, time):\n",
        "\n",
        "        t = self.time_mlp(time)\n",
        "\n",
        "        #print(t.shape, t)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x, t)\n",
        "        x = self.layer2(x, t)\n",
        "        x = self.layer3(x, t)\n",
        "        x = self.layer4(x, t)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        return self._forward_impl(x, time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BZ2nNvIWx0Qv"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        num_classes = 256\n",
        "        cat_cam_mat_feats = num_classes + 16 # ビュー行列の要素数\n",
        "        net_width = 256\n",
        "        self.resnet18 = ResNet(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=num_classes) # 出力は256次元\n",
        "        self.cat_cam_mat_mlp = nn.Sequential(\n",
        "            nn.Linear(cat_cam_mat_feats, net_width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(net_width, net_width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(net_width, net_width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(net_width, net_width),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, images_t, poses, time):\n",
        "        vt = []\n",
        "        for i,img in enumerate(images_t):\n",
        "            outputs_resnet = self.resnet18(img.unsqueeze(0), time)\n",
        "            cat_cam_mat = torch.cat((outputs_resnet, torch.flatten(poses[i])), dim=-1)\n",
        "            # print(torch.flatten(poses[i]).shape)\n",
        "            outputs_cat_cam_mat = self.cat_cam_mat_mlp(cat_cam_mat) # これをすべての視点から集めたい\n",
        "            vt.append(outputs_cat_cam_mat)\n",
        "        vt = torch.stack(vt)\n",
        "        return vt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [],
        "id": "sH87oSTNx0Qv"
      },
      "outputs": [],
      "source": [
        "# class StateEncoder(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         net_width = 256\n",
        "#         self.vt_to_st = nn.Sequential(\n",
        "#             nn.Linear(net_width, net_width),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(net_width, net_width),\n",
        "#             nn.ReLU(),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, vt):\n",
        "#         vt_avg = torch.mean(vt, dim=0) # dim合ってる？\n",
        "#         st = self.vt_to_st(vt_avg)\n",
        "#         return nn.functional.normalize(st,dim=0,p=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "dH44NWcPx0Qw"
      },
      "outputs": [],
      "source": [
        "class VeryTinyNeRFMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.L_pos = 6\n",
        "        self.L_dir = 4\n",
        "        num_classes = 256\n",
        "        pos_enc_feats = 3 + 3 * 2 * self.L_pos + num_classes + 16\n",
        "        dir_enc_feats = 3 + 3 * 2 * self.L_dir\n",
        "        net_width = 256\n",
        "        self.vt_to_st = nn.Sequential(\n",
        "            nn.Linear(net_width, net_width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(net_width, net_width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(net_width, net_width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(net_width, net_width),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.early_mlp = nn.Sequential(\n",
        "            nn.Linear(net_width +32, net_width), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(net_width, net_width + 1), \n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.late_mlp = nn.Sequential(\n",
        "            nn.Linear(net_width + 32, net_width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(net_width, 3),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, vt, images_t, target_idx, poses, xs, ds):\n",
        "        vt_avg = torch.mean(vt, dim=0) # dim合ってる？\n",
        "        st = self.vt_to_st(vt_avg)\n",
        "        \n",
        "        xs_encoded = xs\n",
        "        #for l_pos in range(self.L_pos):\n",
        "        #    xs_encoded.append(torch.sin(2 ** l_pos * torch.pi * xs))\n",
        "        #    xs_encoded.append(torch.cos(2 ** l_pos * torch.pi * xs))\n",
        "\n",
        "        #xs_encoded = torch.cat(xs_encoded, dim=-1)\n",
        "\n",
        "        #ds = ds / ds.norm(p=2, dim=-1).unsqueeze(-1) ここわからん\n",
        "        ds_encoded = ds\n",
        "        #for l_dir in range(self.L_dir):\n",
        "        #    ds_encoded.append(torch.sin(2 ** l_dir * torch.pi * ds))\n",
        "        #    ds_encoded.append(torch.cos(2 ** l_dir * torch.pi * ds))\n",
        "\n",
        "        #ds_encoded = torch.cat(ds_encoded, dim=-1)\n",
        "        #print(ds_encoded.shape)\n",
        "        #print(st.unsqueeze(0).repeat(xs_encoded.shape[0], 1).shape)\n",
        "        xs_encoded = torch.cat((xs_encoded, st.unsqueeze(0).repeat(xs_encoded.shape[0], 1)), dim=-1) # xs_encodedがどんな形してるか？\n",
        "        \n",
        "        outputs = self.early_mlp(xs_encoded)\n",
        "        sigma_is = outputs[:, 0]\n",
        "        c_is = self.late_mlp(torch.cat([outputs[:, 1:], ds_encoded], dim=-1))\n",
        "        return {\"c_is\": c_is, \"sigma_is\": sigma_is, \"st_is\": st}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5fbH1MQGx0Qw"
      },
      "outputs": [],
      "source": [
        "def get_coarse_query_points(ds, N_c, t_i_c_bin_edges, t_i_c_gap, os):\n",
        "    u_is_c = torch.rand(*list(ds.shape[:2]) + [N_c]).to(ds)\n",
        "    t_is_c = t_i_c_bin_edges + u_is_c * t_i_c_gap\n",
        "    r_ts_c = os[..., None, :] + t_is_c[..., :, None] * ds[..., None, :]\n",
        "    return (r_ts_c, t_is_c)\n",
        "\n",
        "\n",
        "def render_radiance_volume(images_t, target_idx, poses, r_ts, ds, chunk_size, F_img_enc, F_dec, t_is, time, encoder):\n",
        "    #print(r_ts.shape)\n",
        "    rs_emb = encoder(r_ts.unsqueeze(3))\n",
        "    #print('\"here1')\n",
        "    #print(rs_emb.shape)\n",
        "    r_ts_flat = rs_emb.reshape((-1, 32))\n",
        "    ds_rep = ds.unsqueeze(2).repeat(1, 1, r_ts.shape[-2], 1)\n",
        "    ds_emb = encoder(ds_rep.unsqueeze(3))\n",
        "    ds_flat = ds_emb.reshape((-1, 32))\n",
        "    c_is = []\n",
        "    sigma_is = []\n",
        "    st_is = []\n",
        "    for chunk_start in range(0, r_ts_flat.shape[0], chunk_size):\n",
        "        r_ts_batch = r_ts_flat[chunk_start : chunk_start + chunk_size]\n",
        "        print(r_ts_batch.shape)\n",
        "        ds_batch = ds_flat[chunk_start : chunk_start + chunk_size]\n",
        "        #print(ds_batch.shape)\n",
        "        vt = F_img_enc(images_t, poses, time)\n",
        "        # st = F_st_enc(vt)\n",
        "        preds = F_dec(vt, images_t, target_idx, poses, r_ts_batch, ds_batch) # ここで実行してる？\n",
        "        c_is.append(preds[\"c_is\"])\n",
        "        sigma_is.append(preds[\"sigma_is\"])\n",
        "        st_is.append(preds[\"st_is\"])\n",
        "\n",
        "    c_is = torch.cat(c_is).reshape(r_ts.shape)\n",
        "    sigma_is = torch.cat(sigma_is).reshape(r_ts.shape[:-1])\n",
        "\n",
        "    delta_is = t_is[..., 1:] - t_is[..., :-1]\n",
        "    one_e_10 = torch.Tensor([1e10]).expand(delta_is[..., :1].shape)\n",
        "    delta_is = torch.cat([delta_is, one_e_10.to(delta_is)], dim=-1)\n",
        "    delta_is = delta_is * ds.norm(dim=-1).unsqueeze(-1)\n",
        "\n",
        "    alpha_is = 1.0 - torch.exp(-sigma_is * delta_is)\n",
        "\n",
        "    T_is = torch.cumprod(1.0 - alpha_is + 1e-10, -1)\n",
        "    T_is = torch.roll(T_is, 1, -1)\n",
        "    T_is[..., 0] = 1.0\n",
        "\n",
        "    w_is = T_is * alpha_is\n",
        "\n",
        "    C_rs = (w_is[..., None] * c_is).sum(dim=-2)\n",
        "\n",
        "    return C_rs, vt, st_is\n",
        "\n",
        "\n",
        "def run_one_iter_of_tiny_nerf(images_t, target_idx, poses, ds, N_c, t_i_c_bin_edges, t_i_c_gap, os, chunk_size, F_img_enc, F_dec, time, encoder):\n",
        "    (r_ts_c, t_is_c) = get_coarse_query_points(ds, N_c, t_i_c_bin_edges, t_i_c_gap, os)\n",
        "    C_rs_c, vt, st = render_radiance_volume(images_t, target_idx, poses, r_ts_c, ds, chunk_size, F_img_enc, F_dec, t_is_c, time, encoder) # ここで実行してる？\n",
        "    return C_rs_c, vt, st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": [],
        "id": "7tmclwM3x0Qw"
      },
      "outputs": [],
      "source": [
        "# class MSETwoView(nn.Module):\n",
        "#     def __init__(self): # パラメータの設定など初期化処理を行う\n",
        "#         super(MSETwoView, self).__init__()\n",
        "        \n",
        "#     def forward(self, C_rs_c, images_t_target):\n",
        "#         print(images_t_target.shape)\n",
        "#         loss = torch.mean((C_rs_c - images_t_target) ** 2, dim=0)\n",
        "#         print(loss.shape)\n",
        "#         return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-_FPwpHUx0Qw"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class MultiresolutionHashEncoder3d(nn.Module):\n",
        "    def __init__(self, l=16, t=2**14, f=2, n_min=16, n_max=100, interpolation='trilinear'):\n",
        "        super().__init__()\n",
        "        self.l = l\n",
        "        self.t = t\n",
        "        self.f = f\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "        b = math.exp((math.log(n_max) - math.log(n_min)) / (l - 1))\n",
        "        self.ns = [int(n_min * (b ** i)) for i in range(l)]\n",
        "\n",
        "        # Prime Numbers from https://github.com/NVlabs/tiny-cuda-nn/blob/ee585fa47e99de4c26f6ae88be7bcb82b9295310/include/tiny-cuda-nn/encodings/grid.h\n",
        "        self.register_buffer('primes', torch.tensor([1, 2654435761, 805459861]))\n",
        "        \n",
        "        self.hash_table = nn.Parameter(\n",
        "            torch.rand([l, t, f], requires_grad=True) * 2e-4 - 1e-4)\n",
        "\n",
        "    @property\n",
        "    def encoded_vector_size(self):\n",
        "        return self.l * self.f\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.permute(3, 4, 0, 1, 2)\n",
        "        b, c, h, w, d = x.size()\n",
        "\n",
        "        def make_grid(x, n):\n",
        "            g = F.max_pool3d(x * n, (h // n, w // n, d // n)).to(dtype=torch.long)\n",
        "            #print(g.shape)\n",
        "            g = g * self.primes.view([3, 1, 1, 1])\n",
        "            g = (g[:,0] ^ g[:,1] ^ g[:,2]) % self.t\n",
        "            #print(g.shape)\n",
        "            return g\n",
        "\n",
        "        grids = [make_grid(x, n) for n in self.ns]\n",
        "        #print(len(grids))\n",
        "        features = [self.hash_table[i, g].permute(0, 4, 1, 2, 3)\n",
        "                    for i, g in enumerate(grids)]\n",
        "        feature_map = torch.hstack([\n",
        "            F.interpolate(f, (h, w, d), mode=self.interpolation)\n",
        "            for f in features\n",
        "        ]) \n",
        "        #print(feature_map.shape)\n",
        "\n",
        "        return feature_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fK5H_8K0x0Qw"
      },
      "outputs": [],
      "source": [
        "class Dynamics(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        net_width = 256\n",
        "        self.dynamics = nn.Sequential(\n",
        "            nn.Linear(net_width, net_width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(net_width, net_width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(net_width, net_width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(net_width, net_width),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, vt):\n",
        "        vt_avg = torch.mean(vt, dim=0) # dim合ってる？\n",
        "        st = self.vt_to_st(vt_avg)\n",
        "        return nn.functional.normalize(st,dim=0,p=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "S0chtN2vx0Qx"
      },
      "outputs": [],
      "source": [
        "#class Position_Emb(nn.Module):\n",
        "#    def __init__(self, encoder, num_planes=64, num_layers=2):\n",
        "#        super().__init__()\n",
        "#        self.enc = encoder\n",
        "        \n",
        "        # 1x1 convolution is equivalent to MLP for a point in the 2D-coordinates\n",
        "#        layers = [nn.Conv3d(encoder.encoded_vector_size, num_planes, 1)]\n",
        "#        for _ in range(num_layers - 2):\n",
        "#            layers += [nn.ReLU(),\n",
        "#                       nn.Conv3d(num_planes, num_planes, 1)]\n",
        "#        layers += [nn.ReLU(),\n",
        "#                   nn.Conv3d(num_planes, 3, 1), #ここあってるかわかんない\n",
        "#                   nn.Sigmoid()]\n",
        "#        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "#    def forward(self, x):\n",
        "#        feature = self.enc(x)\n",
        "#        #print(\"here\")\n",
        "#        out = self.mlp(feature)\n",
        "#        #print(out.shape)\n",
        "#        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "tags": [],
        "id": "vwY31sSvx0Qx"
      },
      "outputs": [],
      "source": [
        "class TimeContrastiveLoss(nn.Module):\n",
        "    def __init__(self): # パラメータの設定など初期化処理を行う\n",
        "        super(TimeContrastiveLoss, self).__init__()\n",
        "        \n",
        "    def forward(self, vt_i, vt_j, vt_cont_i, alpha=0):\n",
        "        loss = torch.norm(vt_i - vt_j, 2)**2 + alpha\n",
        "         # - torch.norm(vt_i - vt_cont_i, 2)**2 \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGIuBvmDBrXd",
        "outputId": "db1dfdba-8dfd-4758-b3d6-12d610a42700"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  train_2box.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgMPFZ7WXDQ3",
        "outputId": "4df5bddd-ae98-4eae-d130-0592c888a5a2",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VeryTinyNeRFMLP(\n",
              "  (vt_to_st): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (7): ReLU()\n",
              "  )\n",
              "  (early_mlp): Sequential(\n",
              "    (0): Linear(in_features=288, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=257, bias=True)\n",
              "    (3): ReLU()\n",
              "  )\n",
              "  (late_mlp): Sequential(\n",
              "    (0): Linear(in_features=288, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=3, bias=True)\n",
              "    (3): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "seed = 9458\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "device = \"cuda:0\"\n",
        "F_image_enc = ImageEncoder().to(device)\n",
        "F_dec = VeryTinyNeRFMLP().to(device)\n",
        "encoder = MultiresolutionHashEncoder3d().to(device)\n",
        "chunk_size = 16384\n",
        "\n",
        "lr = 5e-4\n",
        "optimizer_image_enc = optim.Adam(F_image_enc.parameters(), lr=lr)\n",
        "optimizer_dec = optim.Adam(F_dec.parameters(), lr=lr)\n",
        "optimizer_emb = optim.Adam(encoder.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "# criterion_TC = TimeContrastiveLoss()\n",
        "\n",
        "data_f = \"train_2box.npz\"\n",
        "data = np.load(data_f)\n",
        "#test_data_f = \"test_2box.npz\"\n",
        "#test_data = np.load(test_data_f)\n",
        "\n",
        "images = data[\"images\"][:20] / 255\n",
        "#test_images = test_data[\"images\"][:20] / 255\n",
        "images_t0 = images[0]\n",
        "img_size = images_t0.shape[1]\n",
        "xs = torch.arange(img_size) - (img_size / 2 - 0.5)\n",
        "ys = torch.arange(img_size) - (img_size / 2 - 0.5)\n",
        "(xs, ys) = torch.meshgrid(xs, -ys, indexing=\"xy\")\n",
        "focal = float(data[\"focal\"])\n",
        "pixel_coords = torch.stack([xs, ys, torch.full_like(xs, -focal)], dim=-1)\n",
        "camera_coords = pixel_coords / focal\n",
        "init_ds = camera_coords.to(device)\n",
        "init_o = torch.Tensor(np.array([0, 0, float(data[\"cam_dist\"])])).to(device)\n",
        "\n",
        "poses = torch.Tensor(data[\"poses\"].reshape((-1,4,4))).to(device)\n",
        "#test_poses = torch.Tensor(test_data[\"poses\"].reshape((-1,4,4))).to(device)\n",
        "\n",
        "t_n = 2.0 # near\n",
        "t_f = 10.0 # far\n",
        "N_c = 100\n",
        "t_i_c_gap = (t_f - t_n) / N_c\n",
        "t_i_c_bin_edges = (t_n + torch.arange(N_c) * t_i_c_gap).to(device)\n",
        "\n",
        "psnrs = []\n",
        "iternums = []\n",
        "F_image_enc.train()\n",
        "# F_state_enc.train()\n",
        "F_dec.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "tags": [],
        "id": "oe-u_nkYx0Qx"
      },
      "outputs": [],
      "source": [
        "# # ImageEncoderの学習 -> 要素がすべて0になる\n",
        "# num_iters_image_enc = 100\n",
        "# lr = 5e-4\n",
        "# F_image_enc = ImageEncoder().to(device)\n",
        "# optimizer_image_enc = optim.Adam(F_image_enc.parameters(), lr=lr)\n",
        "# # optimizer_image_enc = optim.SGD(F_image_enc.parameters(), lr=lr)\n",
        "# # 100 step後に5e-5になるように\n",
        "# scheduler = optim.lr_scheduler.ExponentialLR(optimizer_image_enc, gamma=10**(-0.01))\n",
        "# criterion_TC = TimeContrastiveLoss()\n",
        "\n",
        "# history  = []\n",
        "# for iter in range(num_iters_image_enc):\n",
        "#     for t in range(1,images.shape[0]):\n",
        "#         t_cont = random.randint(0, t-1)\n",
        "#         images_t = torch.Tensor(images[t])\n",
        "#         images_t_cont = torch.Tensor(images[t_cont])\n",
        "#         time = torch.Tensor([t]).to(device)\n",
        "#         time_cont = torch.Tensor([t_cont]).to(device)\n",
        "#         while True:\n",
        "#             i = random.randint(0, images_t.shape[0]-1)\n",
        "#             j = random.randint(0, images_t.shape[0]-1)\n",
        "#             if i != j:\n",
        "#                 break\n",
        "#         vt_i = F_image_enc(images_t[i].unsqueeze(0).permute(0,3,1,2).to(device), poses, time)\n",
        "#         vt_j = F_image_enc(images_t[j].unsqueeze(0).permute(0,3,1,2).to(device), poses, time)\n",
        "#         vt_cont_i = F_image_enc(images_t_cont[i].unsqueeze(0).permute(0,3,1,2).to(device), poses, time_cont)\n",
        "#         loss_image_enc = criterion_TC(vt_i, vt_j, vt_cont_i, alpha=0)\n",
        "#         if loss_image_enc > 0:\n",
        "#             optimizer_image_enc.zero_grad()\n",
        "#             loss_image_enc.backward()\n",
        "#             optimizer_image_enc.step()\n",
        "#         # print(f'{iter}, {t}, {t_cont}, Time Contrastive Loss: {loss_image_enc}')\n",
        "        \n",
        "#         history.append(loss_image_enc.item())\n",
        "        \n",
        "#     scheduler.step()\n",
        "#         # print(vt_i)\n",
        "        \n",
        "# plt.plot(history)\n",
        "# #plt.ylim(0, 0.1);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JSs8g_cLx0Qx",
        "outputId": "b57f2f0e-185b-4c0f-d0e9-4a6779ffc98f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n",
            "torch.Size([16384, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/100 [00:04<?, ?it/s]\n",
            "  0%|          | 0/10 [00:04<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-6574be1eb970>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0minit_o\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         C_rs_c,_, _ = run_one_iter_of_tiny_nerf(\n\u001b[0m\u001b[1;32m     39\u001b[0m           \u001b[0mimages_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_img_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_i_c_bin_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_i_c_gap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_image_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-35-2002ff88e208>\u001b[0m in \u001b[0;36mrun_one_iter_of_tiny_nerf\u001b[0;34m(images_t, target_idx, poses, ds, N_c, t_i_c_bin_edges, t_i_c_gap, os, chunk_size, F_img_enc, F_dec, time, encoder)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_one_iter_of_tiny_nerf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_i_c_bin_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_i_c_gap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_img_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mr_ts_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_is_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coarse_query_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_i_c_bin_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_i_c_gap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mC_rs_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_radiance_volume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_ts_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_img_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_is_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ここで実行してる？\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mC_rs_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-2002ff88e208>\u001b[0m in \u001b[0;36mrender_radiance_volume\u001b[0;34m(images_t, target_idx, poses, r_ts, ds, chunk_size, F_img_enc, F_dec, t_is, time, encoder)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mds_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_flat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk_start\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mchunk_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#print(ds_batch.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF_img_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# st = F_st_enc(vt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_ts_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ここで実行してる？\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-5316a712ccd5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images_t, poses, time)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0moutputs_resnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mcat_cam_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_resnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# print(torch.flatten(poses[i]).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-38578a1960d7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, time)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-38578a1960d7>\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x, time)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-60cfc315baf7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b h d e, b h d n -> b h e n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b h c (x y) -> b (h c) x y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         return F.group_norm(\n\u001b[0m\u001b[1;32m    274\u001b[0m             input, self.num_groups, self.weight, self.bias, self.eps)\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected at least 2 dimensions for input tensor but received {input.dim()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_groups\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.75 GiB total capacity; 14.52 GiB already allocated; 832.00 KiB free; 14.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# for file_name in natsorted(os.listdir('dataset')): # dataset分\n",
        "# data_f = \"test (4).npz\"\n",
        "# data = np.load(data_f)\n",
        "# images = data[\"images\"] / 255\n",
        "file_name = 'best2.cpt'\n",
        "min_loss_image_enc = np.inf\n",
        "\n",
        "lr = 5e-3\n",
        "# F_image_enc = ImageEncoder().to(device)\n",
        "# optimizer_image_enc = optim.Adam(F_image_enc.parameters(), lr=lr)\n",
        "criterion_TC = TimeContrastiveLoss()\n",
        "#モデルの定義\n",
        "# F_dec = VeryTinyNeRFMLP().to(device)\n",
        "# optimizer_dec = optim.Adam(F_dec.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "num_loop = 10\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer_image_enc, gamma=10**(-0.002)) # 1000回で0.01倍\n",
        "scheduler_dec = optim.lr_scheduler.ExponentialLR(optimizer_dec, gamma=10**(-0.1))\n",
        "num_iters = 100\n",
        "display_every = 25\n",
        "\n",
        "\n",
        "for loop in tqdm(range(num_loop)): \n",
        "    poses = torch.Tensor(poses).to(device) \n",
        "    for i in tqdm(range(num_iters)):\n",
        "\n",
        "        t = random.randint(0, images.shape[0]-1) #ここあってるかわかんない\n",
        "        images_t = torch.Tensor(images[t]).to(device)\n",
        "        time = torch.Tensor([t]).to(device)\n",
        "\n",
        "        target_img_idx = np.random.randint(images_t.shape[0]-1)\n",
        "        target_pose = poses[target_img_idx].to(device)\n",
        "        R = target_pose[:3, :3]\n",
        "        ds = torch.einsum(\"ij,hwj->hwi\", R, init_ds)\n",
        "        os = (R @ init_o).expand(ds.shape)\n",
        "\n",
        "        C_rs_c,_, _ = run_one_iter_of_tiny_nerf(\n",
        "          images_t.permute(0,3,1,2).to(device), target_img_idx, poses, ds, N_c, t_i_c_bin_edges, t_i_c_gap, os, chunk_size, F_image_enc, F_dec, time, encoder\n",
        "        )\n",
        "\n",
        "        loss_dec = criterion(C_rs_c, images_t[target_img_idx])\n",
        "        optimizer_dec.zero_grad()\n",
        "        optimizer_emb.zero_grad()\n",
        "        loss_dec.backward(retain_graph=True)\n",
        "        optimizer_dec.step()\n",
        "        optimizer_emb.step()\n",
        "\n",
        "\n",
        "        if (i+1) % display_every == 0:\n",
        "            print(f\"Loop: {loop}\")\n",
        "            print(f\"Time Step: {t}\")\n",
        "            print(f\"Iter: {i}\")\n",
        "            print(f\"Loss: {loss_dec.item()}\")\n",
        "            plt.figure(figsize=(4, 4))\n",
        "            plt.imshow(C_rs_c.detach().cpu().numpy())\n",
        "            plt.show()\n",
        "\n",
        "        for t in range(1,images.shape[0]):\n",
        "            t_cont = random.randint(0, t-1)\n",
        "            images_t = torch.Tensor(images[t])\n",
        "            images_t_cont = torch.Tensor(images[t_cont])\n",
        "            time = torch.Tensor([t]).to(device)\n",
        "            time_cont = torch.Tensor([t_cont]).to(device)\n",
        "            while True:\n",
        "                i = random.randint(0, images_t.shape[0]-1)\n",
        "                j = random.randint(0, images_t.shape[0]-1)\n",
        "                if i != j:\n",
        "                    break\n",
        "            vt_i = F_image_enc(images_t[i].unsqueeze(0).permute(0,3,1,2).to(device), poses, time)\n",
        "            vt_j = F_image_enc(images_t[j].unsqueeze(0).permute(0,3,1,2).to(device), poses, time)\n",
        "            vt_cont_i = F_image_enc(images_t_cont[i].unsqueeze(0).permute(0,3,1,2).to(device), poses, time_cont)\n",
        "            loss_image_enc = criterion_TC(vt_i, vt_j, vt_cont_i, alpha=0)\n",
        "            loss_image_enc += loss_dec #loss_decはノルム取るべき？\n",
        "            if loss_image_enc > 0:\n",
        "                optimizer_image_enc.zero_grad()\n",
        "                loss_image_enc.backward(retain_graph=True)\n",
        "                optimizer_image_enc.step()\n",
        "            if loss_image_enc < min_loss_image_enc:\n",
        "                # チェックポイント保存\n",
        "                torch.save({\n",
        "                    'loop': loop,\n",
        "                    'iter': i,\n",
        "                    'time': t,\n",
        "                    'image_encoder_state_dict': F_image_enc.state_dict(),\n",
        "                    'decoder_state_dict': F_dec.state_dict(),\n",
        "                    'opt_img_enc_state_dict': optimizer_image_enc.state_dict(),\n",
        "                    'opt_dec_state_dict': optimizer_dec.state_dict(),\n",
        "                    'loss': loss_image_enc,\n",
        "                }, file_name)\n",
        "                min_loss_image_enc = loss_image_enc\n",
        "            if t == images.shape[0]:\n",
        "                print(f'{iter}, {t}, {t_cont}, Time Contrastive Loss: {loss_image_enc}')\n",
        "        print(scheduler.get_lr()[0])\n",
        "        scheduler.step()\n",
        "    \n",
        "    scheduler_dec.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "KPSOuMlWx0Qy"
      },
      "outputs": [],
      "source": [
        "# 新規視点の描画\n",
        "test_data_f = \"test_2box.npz\"\n",
        "#test_data = np.load(test_data_f)\n",
        "#test_images = test_data[\"images\"][:20] / 255\n",
        "test_poses = torch.Tensor(test_data[\"poses\"].reshape((-1,4,4))).to(device)\n",
        "F_dec_out_st = VeryTinyNeRFMLP().to(device)\n",
        "cpt = torch.load('out_2box.cpt')\n",
        "F_dec_out_st.load_state_dict(cpt['decoder_state_dict'])\n",
        "F_image_enc.eval()\n",
        "F_dec_out_st.eval()\n",
        "for t in range(test_images.shape[0]):\n",
        "    for test_idx in range(test_images.shape[1]):\n",
        "        test_R = torch.Tensor(test_poses[test_idx, :3, :3]).to(device)\n",
        "        test_ds = torch.einsum(\"ij,hwj->hwi\", test_R, init_ds)\n",
        "        test_os = (test_R @ init_o).expand(test_ds.shape)\n",
        "        test_img = torch.Tensor(test_images[t][test_idx]).to(device)\n",
        "        test_pose = torch.Tensor(test_poses[test_idx]).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            C_rs_c_test, vt_test, st = run_one_iter_of_tiny_nerf(\n",
        "                test_img.unsqueeze(0).permute(0,3,1,2), target_img_idx, test_pose, test_ds, N_c, t_i_c_bin_edges, t_i_c_gap, test_os, chunk_size, F_image_enc, F_dec_out_st\n",
        "            )\n",
        "        loss_dec = criterion(C_rs_c_test, test_img)\n",
        "        print(f\"Time: {t}\")\n",
        "        print(f\"View: {test_idx}\")\n",
        "        print(f\"Loss: {loss_dec.item()}\")\n",
        "        print(f\"st: {st}\")\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(C_rs_c.detach().cpu().numpy())\n",
        "        plt.subplot(122)\n",
        "        plt.imshow(test_img.detach().cpu().numpy())\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y4yGfUgx0Qy"
      },
      "outputs": [],
      "source": [
        "test_ds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqvkOMJ5x0Qy"
      },
      "outputs": [],
      "source": [
        "test_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrIV6fjTx0Qy"
      },
      "outputs": [],
      "source": [
        "file_name = 'out_2box.cpt'\n",
        "torch.save({\n",
        "'time': 0,\n",
        "'loop': 4,\n",
        "'image_encoder_state_dict': F_image_enc.state_dict(), # vtまでのencoder\n",
        "'decoder_state_dict': F_dec.state_dict(),\n",
        "'opt_img_enc_state_dict': optimizer_image_enc.state_dict(),\n",
        "'opt_dec_state_dict': optimizer_dec.state_dict(),\n",
        "'loss': loss_dec,\n",
        "}, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abh5DdoAx0Qy"
      },
      "outputs": [],
      "source": [
        "plt.imshow(images[20][19])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZP90QEDx0Qy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}