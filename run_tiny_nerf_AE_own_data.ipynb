{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05wDHgnbiXHq"
   },
   "source": [
    "See my notebook [here](https://colab.research.google.com/drive/1MY6pk3vY7rrYal8oS6_s7zTkGN9lkHQr?usp=sharing) demonstrating how to use my code to train a NeRF model on the `tiny_nerf_data.npz` file used by the original NeRF authors in their notebook [here](https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKJ6NMd642-Y",
    "outputId": "8c52c241-c870-4caf-f1be-c1184449d930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "eyiZCUkSW5HH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "# ResNet18\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        # self.fc = nn.Linear(2048*4, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                # if isinstance(m, Bottleneck):\n",
    "                #     nn.init.constant_(m.bn3.weight, 0)\n",
    "                # elif isinstance(m, BasicBlock):\n",
    "                nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "# camera-view行列と結合\n",
    "# MLPに通す\n",
    "# 得られた視点ごとの特徴量を平均化\n",
    "# MLPに通してL2正則化→s_tが得られる\n",
    "\n",
    "\n",
    "# ここまでResNet18\n",
    "\n",
    "def get_coarse_query_points(ds, N_c, t_i_c_bin_edges, t_i_c_gap, os):\n",
    "    u_is_c = torch.rand(*list(ds.shape[:2]) + [N_c]).to(ds)\n",
    "    t_is_c = t_i_c_bin_edges + u_is_c * t_i_c_gap\n",
    "    r_ts_c = os[..., None, :] + t_is_c[..., :, None] * ds[..., None, :]\n",
    "    return (r_ts_c, t_is_c)\n",
    "\n",
    "\n",
    "def render_radiance_volume(img, target_pose, r_ts, ds, chunk_size, F, t_is):\n",
    "    r_ts_flat = r_ts.reshape((-1, 3))\n",
    "    ds_rep = ds.unsqueeze(2).repeat(1, 1, r_ts.shape[-2], 1)\n",
    "    ds_flat = ds_rep.reshape((-1, 3))\n",
    "    c_is = []\n",
    "    sigma_is = []\n",
    "    for chunk_start in range(0, r_ts_flat.shape[0], chunk_size):\n",
    "        r_ts_batch = r_ts_flat[chunk_start : chunk_start + chunk_size]\n",
    "        ds_batch = ds_flat[chunk_start : chunk_start + chunk_size]\n",
    "        preds = F(img, target_pose, r_ts_batch, ds_batch) # ここで実行してる？\n",
    "        c_is.append(preds[\"c_is\"])\n",
    "        sigma_is.append(preds[\"sigma_is\"])\n",
    "\n",
    "    c_is = torch.cat(c_is).reshape(r_ts.shape)\n",
    "    sigma_is = torch.cat(sigma_is).reshape(r_ts.shape[:-1])\n",
    "\n",
    "    delta_is = t_is[..., 1:] - t_is[..., :-1]\n",
    "    one_e_10 = torch.Tensor([1e10]).expand(delta_is[..., :1].shape)\n",
    "    delta_is = torch.cat([delta_is, one_e_10.to(delta_is)], dim=-1)\n",
    "    delta_is = delta_is * ds.norm(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    alpha_is = 1.0 - torch.exp(-sigma_is * delta_is)\n",
    "\n",
    "    T_is = torch.cumprod(1.0 - alpha_is + 1e-10, -1)\n",
    "    T_is = torch.roll(T_is, 1, -1)\n",
    "    T_is[..., 0] = 1.0\n",
    "\n",
    "    w_is = T_is * alpha_is\n",
    "\n",
    "    C_rs = (w_is[..., None] * c_is).sum(dim=-2)\n",
    "\n",
    "    return C_rs\n",
    "\n",
    "\n",
    "def run_one_iter_of_tiny_nerf(img, target_pose, ds, N_c, t_i_c_bin_edges, t_i_c_gap, os, chunk_size, F_c):\n",
    "    (r_ts_c, t_is_c) = get_coarse_query_points(ds, N_c, t_i_c_bin_edges, t_i_c_gap, os)\n",
    "    C_rs_c = render_radiance_volume(img, target_pose, r_ts_c, ds, chunk_size, F_c, t_is_c) # ここで実行してる？\n",
    "    return C_rs_c\n",
    "\n",
    "\n",
    "class VeryTinyNeRFMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.L_pos = 6\n",
    "        self.L_dir = 4\n",
    "        num_classes = 256\n",
    "        cat_cam_mat_feats = num_classes + 16\n",
    "        pos_enc_feats = 3 + 3 * 2 * self.L_pos + num_classes + 16\n",
    "        dir_enc_feats = 3 + 3 * 2 * self.L_dir\n",
    "\n",
    "        net_width = 256\n",
    "        self.resnet18 = ResNet(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=num_classes) # 出力は256次元\n",
    "        \n",
    "        self.cat_cam_mat_mlp = nn.Sequential(\n",
    "            nn.Linear(cat_cam_mat_feats, net_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(net_width, net_width),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.vt_to_st = nn.Sequential(\n",
    "            nn.Linear(net_width, net_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(net_width, net_width),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.early_mlp = nn.Sequential(\n",
    "            nn.Linear(net_width + self.L_pos*2*3+3, net_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(net_width, net_width + 1), # なんで+1？\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.late_mlp = nn.Sequential(\n",
    "            nn.Linear(net_width + dir_enc_feats, net_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(net_width, 3),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, images, poses, xs, ds):\n",
    "        vt = []\n",
    "        # print(xs.shape)\n",
    "        xs_encoded = [xs]\n",
    "        for l_pos in range(self.L_pos):\n",
    "            xs_encoded.append(torch.sin(2 ** l_pos * torch.pi * xs))\n",
    "            xs_encoded.append(torch.cos(2 ** l_pos * torch.pi * xs))\n",
    "\n",
    "        xs_encoded = torch.cat(xs_encoded, dim=-1)\n",
    "        # print(ds.shape)\n",
    "\n",
    "        ds = ds / ds.norm(p=2, dim=-1).unsqueeze(-1)\n",
    "        ds_encoded = [ds]\n",
    "        for l_dir in range(self.L_dir):\n",
    "            ds_encoded.append(torch.sin(2 ** l_dir * torch.pi * ds))\n",
    "            ds_encoded.append(torch.cos(2 ** l_dir * torch.pi * ds))\n",
    "\n",
    "        ds_encoded = torch.cat(ds_encoded, dim=-1)\n",
    "        \n",
    "        for i,img in enumerate(images):\n",
    "            outputs_resnet = self.resnet18(img.unsqueeze(0))\n",
    "            cat_cam_mat = torch.cat((outputs_resnet, torch.flatten(poses[i])), dim=-1)\n",
    "            outputs_cat_cam_mat = self.cat_cam_mat_mlp(cat_cam_mat) # これをすべての視点から集めたい\n",
    "            vt.append(outputs_cat_cam_mat)\n",
    "        # print(vt)\n",
    "        \n",
    "        vt = torch.stack([vt], dim=0)\n",
    "        vt_avg = torch.mean(vt, dim=0) # dim合ってる？\n",
    "        st = self.vt_to_st(vt_avg)\n",
    "\n",
    "        # print(outputs_cat_cam_mat.shape)\n",
    "        xs_encoded = torch.cat((xs_encoded, st.unsqueeze(0).repeat(xs_encoded.shape[0], 1)), dim=-1) # xs_encodedがどんな形してるか？\n",
    "        outputs = self.early_mlp(xs_encoded)\n",
    "        sigma_is = outputs[:, 0]\n",
    "        c_is = self.late_mlp(torch.cat([outputs[:, 1:], ds_encoded], dim=-1))\n",
    "        return {\"c_is\": c_is, \"sigma_is\": sigma_is}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vQLmPp_-nzq",
    "outputId": "fb9d21be-2e8e-4321-da7f-3375dddb1d8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_f = \"test (1).npz\"\n",
    "data = np.load(data_f)\n",
    "len(data['images'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rgMPFZ7WXDQ3",
    "outputId": "4d71a3e4-23ca-46f3-f037-966c6b5f793b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VeryTinyNeRFMLP(\n",
       "  (resnet18): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (cat_cam_mat_mlp): Sequential(\n",
       "    (0): Linear(in_features=272, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (vt_to_st): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (early_mlp): Sequential(\n",
       "    (0): Linear(in_features=295, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=257, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (late_mlp): Sequential(\n",
       "    (0): Linear(in_features=283, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=3, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 9458\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "F_c = VeryTinyNeRFMLP().to(device)\n",
    "chunk_size = 16384\n",
    "\n",
    "lr = 5e-3\n",
    "optimizer = optim.Adam(F_c.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "data_f = \"test (1).npz\"\n",
    "data = np.load(data_f)\n",
    "\n",
    "images = data[\"images\"][0] / 255\n",
    "img_size = images.shape[1]\n",
    "xs = torch.arange(img_size) - (img_size / 2 - 0.5)\n",
    "ys = torch.arange(img_size) - (img_size / 2 - 0.5)\n",
    "(xs, ys) = torch.meshgrid(xs, -ys, indexing=\"xy\")\n",
    "focal = float(data[\"focal\"])\n",
    "pixel_coords = torch.stack([xs, ys, torch.full_like(xs, -focal)], dim=-1)\n",
    "camera_coords = pixel_coords / focal\n",
    "init_ds = camera_coords.to(device)\n",
    "init_o = torch.Tensor(np.array([0, 0, float(data[\"cam_dist\"])])).to(device)\n",
    "\n",
    "test_idx = 30\n",
    "plt.imshow(images[test_idx])\n",
    "plt.show()\n",
    "test_img = torch.Tensor(images[test_idx]).to(device)\n",
    "poses = torch.Tensor(data[\"poses\"].reshape((-1,4,4))).to(device)\n",
    "test_pose = torch.Tensor(poses[test_idx]).to(device)\n",
    "test_R = torch.Tensor(poses[test_idx, :3, :3]).to(device)\n",
    "test_ds = torch.einsum(\"ij,hwj->hwi\", test_R, init_ds)\n",
    "test_os = (test_R @ init_o).expand(test_ds.shape)\n",
    "\n",
    "t_n = 1.0\n",
    "t_f = 4.0\n",
    "N_c = 32\n",
    "t_i_c_gap = (t_f - t_n) / N_c\n",
    "t_i_c_bin_edges = (t_n + torch.arange(N_c) * t_i_c_gap).to(device)\n",
    "\n",
    "train_idxs = np.arange(len(images)) != test_idx # test_idxだけ除去(1枚)\n",
    "images = torch.Tensor(images[train_idxs])\n",
    "poses = torch.Tensor(poses[train_idxs])\n",
    "psnrs = []\n",
    "iternums = []\n",
    "num_iters = 20000\n",
    "display_every = 100\n",
    "F_c.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t6yJtLhVXKfG",
    "outputId": "c51e5874-af16-445f-b9df-7a301eba5efa"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m ds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mij,hwj->hwi\u001b[39m\u001b[38;5;124m\"\u001b[39m, R, init_ds)\n\u001b[0;32m      7\u001b[0m os \u001b[38;5;241m=\u001b[39m (R \u001b[38;5;241m@\u001b[39m init_o)\u001b[38;5;241m.\u001b[39mexpand(ds\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 9\u001b[0m C_rs_c \u001b[38;5;241m=\u001b[39m \u001b[43mrun_one_iter_of_tiny_nerf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_img_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_i_c_bin_edges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_i_c_gap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_c\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(C_rs_c, images\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[1;32mIn[110], line 207\u001b[0m, in \u001b[0;36mrun_one_iter_of_tiny_nerf\u001b[1;34m(img, target_pose, ds, N_c, t_i_c_bin_edges, t_i_c_gap, os, chunk_size, F_c)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_one_iter_of_tiny_nerf\u001b[39m(img, target_pose, ds, N_c, t_i_c_bin_edges, t_i_c_gap, os, chunk_size, F_c):\n\u001b[0;32m    206\u001b[0m     (r_ts_c, t_is_c) \u001b[38;5;241m=\u001b[39m get_coarse_query_points(ds, N_c, t_i_c_bin_edges, t_i_c_gap, os)\n\u001b[1;32m--> 207\u001b[0m     C_rs_c \u001b[38;5;241m=\u001b[39m \u001b[43mrender_radiance_volume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_pose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_ts_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_is_c\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# ここで実行してる？\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m C_rs_c\n",
      "Cell \u001b[1;32mIn[110], line 180\u001b[0m, in \u001b[0;36mrender_radiance_volume\u001b[1;34m(img, target_pose, r_ts, ds, chunk_size, F, t_is)\u001b[0m\n\u001b[0;32m    178\u001b[0m r_ts_batch \u001b[38;5;241m=\u001b[39m r_ts_flat[chunk_start : chunk_start \u001b[38;5;241m+\u001b[39m chunk_size]\n\u001b[0;32m    179\u001b[0m ds_batch \u001b[38;5;241m=\u001b[39m ds_flat[chunk_start : chunk_start \u001b[38;5;241m+\u001b[39m chunk_size]\n\u001b[1;32m--> 180\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_pose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_ts_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_batch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# ここで実行してる？\u001b[39;00m\n\u001b[0;32m    181\u001b[0m c_is\u001b[38;5;241m.\u001b[39mappend(preds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_is\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    182\u001b[0m sigma_is\u001b[38;5;241m.\u001b[39mappend(preds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigma_is\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lth\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[110], line 275\u001b[0m, in \u001b[0;36mVeryTinyNeRFMLP.forward\u001b[1;34m(self, images, poses, xs, ds)\u001b[0m\n\u001b[0;32m    272\u001b[0m     vt\u001b[38;5;241m.\u001b[39mappend(outputs_cat_cam_mat)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# print(vt)\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m vt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m vt_avg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(vt, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# dim合ってる？\u001b[39;00m\n\u001b[0;32m    277\u001b[0m st \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvt_to_st(vt_avg)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "for i in range(num_iters):\n",
    "    target_img_idx = np.random.randint(images.shape[0])\n",
    "    target_pose = poses[target_img_idx].to(device)\n",
    "    R = target_pose[:3, :3]\n",
    "\n",
    "    ds = torch.einsum(\"ij,hwj->hwi\", R, init_ds)\n",
    "    os = (R @ init_o).expand(ds.shape)\n",
    "\n",
    "    C_rs_c = run_one_iter_of_tiny_nerf(\n",
    "        images[target_img_idx].unsqueeze(0).permute(0,3,1,2).to(device), poses, ds, N_c, t_i_c_bin_edges, t_i_c_gap, os, chunk_size, F_c\n",
    "    )\n",
    "    loss = criterion(C_rs_c, images.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % display_every == 0:\n",
    "        F_c.eval()\n",
    "        with torch.no_grad():\n",
    "            C_rs_c = run_one_iter_of_tiny_nerf(\n",
    "                test_img.unsqueeze(0).permute(0,3,1,2).to(device), test_pose, test_ds, N_c, t_i_c_bin_edges, t_i_c_gap, test_os, chunk_size, F_c\n",
    "            )\n",
    "\n",
    "        loss = criterion(C_rs_c, test_img)\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        psnr = -10.0 * torch.log10(loss)\n",
    "\n",
    "        psnrs.append(psnr.item())\n",
    "        iternums.append(i)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(C_rs_c.detach().cpu().numpy())\n",
    "        plt.title(f\"Iteration {i}\")\n",
    "        plt.subplot(122)\n",
    "        plt.plot(iternums, psnrs)\n",
    "        plt.title(\"PSNR\")\n",
    "        plt.show()\n",
    "\n",
    "        F_c.train()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNO_pdwHAuOi",
    "outputId": "4fd0eae0-d69b-4e2e-9c06-0f413afb1301"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "mqfW8X7IAwis",
    "outputId": "36b96fe5-0030-448c-ac34-c59c7a7a1efc"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-45d66a57f373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xs_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "print(xs_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2FPfm1dERrv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
